{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_files = {}\n",
    "usfds1_dataframes = list()\n",
    "time_intervals = ((1, 1996, 2000), (2, 2001, 2005), (3, 2006, 2010), (4, 2011, 2014))\n",
    "\n",
    "for time_interval in time_intervals:\n",
    "    filename = 'CX_DataScientistAssessment_20170713/{}.US_Fare_Dmd_{}_{}.xlsx'.format(*time_interval)\n",
    "    usfd_file = pd.ExcelFile(filename)\n",
    "    if usfd_file.sheet_names != ['Sheet1', 'Sheet2', 'Sheet3']:\n",
    "        raise\n",
    "    for dfsheet in [usfd_file.parse(_) for _ in ['Sheet2', 'Sheet3']]:\n",
    "        if not dfsheet.empty:\n",
    "            raise\n",
    "    usfds1 = usfd_file.parse('Sheet1')\n",
    "    usfds1['mincity'] = usfds1.apply(lambda _: min(_['City1'], _['City2']), axis=1)\n",
    "    usfds1['maxcity'] = usfds1.apply(lambda _: max(_['City1'], _['City2']), axis=1)\n",
    "    usfds1[['City1','City2']] = usfds1[['mincity', 'maxcity']]\n",
    "    usfds1=usfds1.drop(['mincity', 'maxcity'], axis=1)\n",
    "    usfds1['source'] = time_interval[0]\n",
    "    usfds1_dataframes.append(usfds1)\n",
    "    \n",
    "try:\n",
    "    for df in usfds1_dataframes:\n",
    "        if not all(usfds1_dataframes[0].columns == df.columns):\n",
    "            raise\n",
    "except ValueError:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd = pd.concat(usfds1_dataframes, axis=0, ignore_index=True)\n",
    "usfd.to_pickle('usfd.pickle')\n",
    "usfd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_type_file = pd.ExcelFile('CX_DataScientistAssessment_20170713/AirlineType.xlsx')\n",
    "airline_types = airline_type_file.parse('AirlineType')\n",
    "airline_types.index = airline_types['Airline']\n",
    "del airline_types['Airline']\n",
    "#airline_types[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markets_file = pd.ExcelFile('CX_DataScientistAssessment_20170713/2011_2014_Markets.xlsx')\n",
    "markets = markets_file.parse('Sheet1')\n",
    "markets['mincity'] = markets.apply(lambda _: min(_['city1'], _['city2']), axis=1)\n",
    "markets['maxcity'] = markets.apply(lambda _: max(_['city1'], _['city2']), axis=1)\n",
    "markets[['City1','City2']] = markets[['mincity', 'maxcity']]\n",
    "markets=markets.drop(['mincity', 'maxcity', 'city1', 'city2'], axis=1)\n",
    "#markets.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Showing that there are indeed missing values\n",
    "grouped_markets = markets.groupby(['City1', 'City2']).mean()\n",
    "grouped_markets.iloc[((grouped_markets.index.get_level_values('City1') == 'Albuquerque, NM') \n",
    "                     & (grouped_markets.index.get_level_values('City2') == 'Salt Lake City, UT'))\n",
    "                    |((grouped_markets.index.get_level_values('City2') == 'Albuquerque, NM') \n",
    "                     & (grouped_markets.index.get_level_values('City1') == 'Salt Lake City, UT'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_airport_distance = markets.groupby(['City1', 'City2']).mean()\n",
    "only_average_airport_distance = average_airport_distance[['distance']]\n",
    "#only_average_airport_distance[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_w_distance = pd.merge(left=usfd, right=only_average_airport_distance, how='left',\n",
    "                           left_on=['City1', 'City2'], \n",
    "                           right_index=True)\n",
    "#usfd_w_distance[:5]\n",
    "#usfd_w_distance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_w_distance_mssng = usfd_w_distance[pd.isnull(usfd_w_distance['distance'])]\n",
    "lc = list(usfd_w_distance_mssng.groupby(['City1', 'City2']).size().index)\n",
    "relevant_cities = set([_[0] for _ in lc ] + [_[1] for _ in lc])\n",
    "relevant_cities.add('Dallas/Fort Worth, TX')\n",
    "with open('relevant_cities.json','w') as f:\n",
    "    f.write(json.dumps(list(relevant_cities)))\n",
    "with open('citydict.json','r') as f:\n",
    "    citydict = json.loads(f.read())\n",
    "\n",
    "def distance_fix(row, cdict):\n",
    "    if pd.isnull(row['distance'])\\\n",
    "        and row['City1'] in cdict \\\n",
    "        and row['City2'] in cdict:\n",
    "            return vincenty(cdict[row['City1']], cdict[row['City2']]).miles\n",
    "    return row['distance']\n",
    "usfd_w_distance['distance'] = usfd_w_distance.apply(lambda row: distance_fix(row, citydict), axis=1)\n",
    "#usfd_w_distance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_w_distance_type = pd.merge(left=usfd_w_distance, right=airline_types, how='left',\n",
    "                                left_on=['Airline'], right_index=True)\n",
    "#usfd_w_distance_type[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_dict = {'TW': 'Legacy', #TWA\n",
    "                'QQ': 'Regional', #Reno airlines\n",
    "                'QX': 'Regional' #Horizon/Alaska\n",
    "               }\n",
    "def airline_type_fix(row):\n",
    "    if pd.isnull(row['AirlineType']) and row['Airline'] in airline_dict:\n",
    "            return airline_dict[row['Airline']]\n",
    "    return row['AirlineType']\n",
    "usfd_w_distance_type['AirlineType'] = usfd_w_distance_type.apply(airline_type_fix, axis=1)\n",
    "usfd_w_distance_type_ok = usfd_w_distance_type[(pd.isnull(usfd_w_distance_type['AirlineType'])==False) & \n",
    "                                               (pd.isnull(usfd_w_distance_type['distance'])==False)].copy()\n",
    "usfd_w_distance_type_ok.to_pickle('usfd_w_distance_type_ok.pickle')\n",
    "#usfd_w_distance_type_ok.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_wdt = usfd_w_distance_type_ok.copy()\n",
    "passed_time = lambda _: _['Year'] - 1996 + _['Quarter']/4 - .25\n",
    "usfd_wdt['time'] = usfd_wdt.apply(passed_time, axis=1)\n",
    "fare_spread = lambda _: _['MaxFare'] / _['MinFare']\n",
    "usfd_wdt['fare_spread'] = usfd_wdt.apply(fare_spread, axis=1)\n",
    "fare_per_km = lambda _: _['AvgFare'] / _['distance']\n",
    "usfd_wdt['fare_per_km'] = usfd_wdt.apply(fare_per_km, axis=1)\n",
    "against_market = lambda _: _['AvgFare'] / _['Avg_mkt_fare']\n",
    "usfd_wdt['against_market'] = usfd_wdt.apply(against_market, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_type_presence = usfd_wdt.groupby(['Year', 'Quarter', 'City1', 'City2', 'AirlineType']).sum()\n",
    "airline_type_presence_lcc = airline_type_presence.iloc[airline_type_presence.index.get_level_values('AirlineType') \n",
    "                                                       == 'LCC'].copy()\n",
    "airline_type_presence_lcc = airline_type_presence_lcc[['MarketShare']]\n",
    "airline_type_presence_lcc.columns = ['LCC_market_share']\n",
    "airline_type_presence_lcc['has_lcc_presence'] = 1\n",
    "has_strong_lcc = lambda _: 1 if _['LCC_market_share']>.3 else 0\n",
    "airline_type_presence_lcc['has_strong_lcc'] = airline_type_presence_lcc.apply(has_strong_lcc, axis=1)\n",
    "airline_type_presence_lcc.index = airline_type_presence_lcc.index.droplevel(['AirlineType'])\n",
    "#airline_type_presence_lcc[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_competition = usfd_wdt.groupby(['Year', 'Quarter', 'City1', 'City2']).size()\n",
    "airline_competition_frame = airline_competition.to_frame()\n",
    "airline_competition_frame.columns = ['competitors']\n",
    "#airline_competition_frame[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "market_size = usfd_wdt.groupby(['Year', 'Quarter', 'City1', 'City2']).sum()['Passengers'].to_frame()\n",
    "market_size.columns = ['market_size']\n",
    "#market_size[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_extended_0 = pd.merge(left=usfd_wdt, right=airline_type_presence_lcc, how='left',\n",
    "                                left_on=['Year', 'Quarter', 'City1', 'City2'], right_index=True)\n",
    "usfd_extended_0.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_extended_1 = pd.merge(left=usfd_extended_0, right=airline_competition_frame, how='left',\n",
    "                                left_on=['Year', 'Quarter', 'City1', 'City2'], right_index=True)\n",
    "#usfd_extended_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_extended_2 = pd.merge(left=usfd_extended_1, right=market_size, how='left',\n",
    "                                left_on=['Year', 'Quarter', 'City1', 'City2'], right_index=True)\n",
    "#usfd_extended_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_onehot = pd.concat([ usfd_extended_2,\n",
    "                           pd.get_dummies(usfd_extended_2[['AirlineType']]),\n",
    "                           pd.get_dummies(usfd_extended_2['Quarter'])],\n",
    "                         axis=1).drop(['City1ID', 'City1', 'City2ID', 'City2', \n",
    "                                              'AirlineID', 'Airline', 'AirlineType'], axis=1)\n",
    "\n",
    "usfd_city_onehot = pd.concat([ usfd_extended_2, \n",
    "                           pd.get_dummies(usfd_extended_2[['City1']]), \n",
    "                           pd.get_dummies(usfd_extended_2[['City2']]), \n",
    "                           pd.get_dummies(usfd_extended_2[['AirlineType']]),\n",
    "                           pd.get_dummies(usfd_extended_2['Quarter'])],\n",
    "                         axis=1).drop(['City1ID', 'City1', 'City2ID', 'City2', \n",
    "                                              'AirlineID', 'Airline', 'AirlineType'], axis=1)\n",
    "\n",
    "usfd_onehot.to_pickle('usfd_onehot.pickle')\n",
    "#usfd_onehot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_extended_additional = usfd_extended_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "western_metro = {'Los Angeles, CA (Metropolitan Area)', 'San Francisco, CA (Metropolitan Area)', '',}\n",
    "eastern_metro = {'Boston, MA (Metropolitan Area)', 'New York City, NY (Metropolitan Area)', \n",
    "                 'Miami, FL (Metropolitan Area)', 'Washington, DC (Metropolitan Area)'}\n",
    "other_metro = {'Chicago, IL', 'Dallas/Fort Worth, TX', 'Atlanta, GA (Metropolitan Area)',  'Houston, TX'}\n",
    "all_metro = western_metro | eastern_metro | other_metro\n",
    "\n",
    "def is_transcon(row):\n",
    "    return row['City1'] in western_metro and row['City2'] in eastern_metro \\\n",
    "        or row['City2'] in western_metro and row['City1'] in eastern_metro\n",
    "    \n",
    "def is_metro_metro(row):\n",
    "    return row['City1'] in all_metro and row['City2'] in all_metro\n",
    "\n",
    "def is_metro(row):\n",
    "    return row['City1'] in all_metro or row['City2'] in all_metro\n",
    "\n",
    "usfd_extended_additional['is_metro'] = usfd_extended_additional.apply(is_metro, axis=1)\n",
    "usfd_extended_additional['is_metro_metro'] = usfd_extended_additional.apply(is_metro_metro, axis=1)\n",
    "usfd_extended_additional['is_transcon'] = usfd_extended_additional.apply(is_transcon, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kerosene_file = pd.ExcelFile('CX_DataScientistAssessment_20170713/EMA_EPPK_PWG_NUS_DPGm.xls')\n",
    "kerosene = kerosene_file.parse('Data 1')[2:]\n",
    "kerosene.columns=['date','price']\n",
    "kerosene['year'] = kerosene.date.dt.year\n",
    "kerosene['quarter'] = kerosene.date.dt.quarter\n",
    "kerosene.drop(['date'], axis=1)\n",
    "kerosene['price'] = np.float32(kerosene['price'])\n",
    "kerosene_by_quarter = kerosene.groupby(['year', 'quarter']).mean()\n",
    "usfd_extended_additional_kerosene = pd.merge(how='left', left=usfd_extended_additional, right=kerosene_by_quarter, \n",
    "                                            left_on=['Year','Quarter'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usfd_additional_onehot = pd.concat([ usfd_extended_additional_kerosene, \n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene[['AirlineType']]),\n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene['Quarter'])],\n",
    "                         axis=1).drop(['City1ID', 'City1', 'City2ID', 'City2', \n",
    "                                              'AirlineID', 'Airline', 'AirlineType'], axis=1)\n",
    "\n",
    "usfd_additional_city_onehot = pd.concat([ usfd_extended_additional_kerosene, \n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene[['City1']]), \n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene[['City2']]), \n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene[['AirlineType']]),\n",
    "                           pd.get_dummies(usfd_extended_additional_kerosene['Quarter'])],\n",
    "                         axis=1).drop(['City1ID', 'City1', 'City2ID', 'City2', \n",
    "                                              'AirlineID', 'Airline', 'AirlineType'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_df_pickle(df, filename):\n",
    "    data_df = df.drop(['MaxShare'], axis=1)\n",
    "    target_df = df[['MaxShare']]\n",
    "    data_df.to_pickle('{}_data.pickle'.format(filename))\n",
    "    target_df.to_pickle('{}_target.pickle'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_by_time(df, name):\n",
    "    train = df[df['Year'].isin(list(range(1996, 2011)))].drop(['Year'], axis=1)\n",
    "    write_df_pickle(train, '{}_by_time_train'.format(name))\n",
    "    test = df[df['Year'].isin(list(range(2011, 2016)))].drop(['Year'], axis=1)\n",
    "    write_df_pickle(test, '{}_by_time_test'.format(name))\n",
    "write_by_time(usfd_onehot, 'usfd_onehot')\n",
    "write_by_time(usfd_city_onehot, 'usfd_city_onehot')\n",
    "write_by_time(usfd_additional_onehot, 'usfd_additional_onehot')\n",
    "write_by_time(usfd_additional_city_onehot, 'usfd_additional_city_onehot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_train_test_split(df, test_share):\n",
    "    test_indices = random.sample(list(df.index), int(len(df)*test_share))\n",
    "    train_indices = [_ for _ in df.index if _ not in test_indices]\n",
    "    return df.loc[train_indices], df.loc[test_indices]\n",
    "\n",
    "def write_random_20(df, name):\n",
    "    random_train_set, random_test_set = random_train_test_split(df, .2)\n",
    "    write_df_pickle(random_train_set, '{}_random_20_train'.format(name))\n",
    "    write_df_pickle(random_test_set, '{}_random_20_test'.format(name))\n",
    "write_random_20(usfd_onehot, 'usfd_onehot')\n",
    "write_random_20(usfd_city_onehot, 'usfd_city_onehot')\n",
    "write_random_20(usfd_additional_onehot, 'usfd_additional_onehot')\n",
    "write_random_20(usfd_additional_city_onehot, 'usfd_additional_city_onehot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
